---
phase: 02-python-ml-service
plan: 02
type: execute
wave: 2
depends_on: [02-01]
files_modified:
  - python-service/services/pdf_extractor.py
  - python-service/services/chunker.py
  - python-service/services/__init__.py
  - python-service/api/pdf.py
  - python-service/models/schemas.py
  - python-service/models/__init__.py
  - python-service/requirements.txt
autonomous: true

must_haves:
  truths:
    - "PDF files can be uploaded via HTTP POST endpoint"
    - "Text is extracted from PDF pages with position tracking"
    - "Text is chunked into ~500 token pieces with token counting"
    - "Chunks include metadata (doc_id, page_num, chunk_index, offset)"
  artifacts:
    - path: "python-service/services/pdf_extractor.py"
      provides: "PDF text extraction with position tracking"
      exports: ["extract_pdf_pages"]
    - path: "python-service/services/chunker.py"
      provides: "Token-aware text chunking"
      exports: ["chunk_text_by_tokens"]
    - path: "python-service/api/pdf.py"
      provides: "PDF upload and processing endpoint"
      exports: ["POST /api/pdf/upload"]
    - path: "python-service/requirements.txt"
      provides: "Python dependencies"
      contains: ["PyMuPDF", "tiktoken"]
  key_links:
    - from: "python-service/api/pdf.py"
      to: "python-service/services/pdf_extractor.py"
      via: "import statement"
      pattern: "from.*pdf_extractor.*import"
    - from: "python-service/api/pdf.py"
      to: "python-service/services/chunker.py"
      via: "import statement"
      pattern: "from.*chunker.*import"
    - from: "python-service/services/chunker.py"
      to: "tiktoken"
      via: "import statement"
      pattern: "import tiktoken"
---

<objective>
PDF text extraction with token-aware chunking via HTTP upload endpoint.

Purpose: Enable PDF processing pipeline that extracts text and prepares it for vector storage.
Output: Working PDF upload endpoint that returns extracted, chunked text with metadata.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/phases/02-python-ml-service/02-CONTEXT.md
@.planning/phases/02-python-ml-service/02-RESEARCH.md
@.planning/phases/02-python-ml-service/02-01-SUMMARY.md

# Phase 1 Summary for context
@.planning/phases/01-electron-foundation/01-03-SUMMARY.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create PDF extraction service with PyMuPDF</name>
  <files>
    python-service/services/pdf_extractor.py
    python-service/services/__init__.py
  </files>
  <action>
    Create PDF text extraction service:

    **python-service/services/__init__.py:**
    - Empty init file for services package

    **python-service/services/pdf_extractor.py:**
    - Import fitz (PyMuPDF) and typing (List, Dict)
    - Implement extract_pdf_pages(pdf_path: str) function:
      - Open PDF with fitz.open(pdf_path)
      - Iterate through pages with enumerate(doc, start=1) for 1-based page numbers
      - For each page: extract text with page.get_text("text")
      - Track global character offset (starts at 0, increments by len(text) each page)
      - Build pages_data list with dicts:
        ```python
        {
            "page_num": page_num,
            "text": text,
            "offset": global_offset,
            "length": len(text)
        }
        ```
      - Extract metadata from doc.metadata:
        - title, author, creationDate (use empty string as default)
      - Return dict: {"pages": pages_data, "metadata": metadata_dict}
      - Close doc explicitly with doc.close() to prevent memory leaks
    - Add docstring explaining position tracking for PDF navigation
    - Wrap fitz operations in try/except to handle corrupted PDFs
    - On error, raise ValueError with descriptive message

    Do NOT use PyPDF2 (PyMuPDF is faster and more accurate for position tracking).
  </action>
  <verify>
    Create test PDF or use existing: Verify function runs without error
    Check return value: Contains 'pages' list and 'metadata' dict
    Validate page_num: Starts at 1, increments correctly
    Validate offset: Each page offset equals sum of previous page lengths
  </verify>
  <done>
    extract_pdf_pages() returns structured data with pages and metadata, positions tracked correctly.
  </done>
</task>

<task type="auto">
  <name>Task 2: Create token-aware chunking service</name>
  <files>
    python-service/services/chunker.py
  </files>
  <action>
    Create token-aware chunking service:

    **python-service/services/chunker.py:**
    - Import tiktoken and typing (List, Dict)
    - Define constants: MAX_TOKENS = 500, OVERLAP = 50
    - Implement chunk_text_by_tokens function:
      ```python
      def chunk_text_by_tokens(
          pages: List[Dict],
          max_tokens: int = 500,
          overlap: int = 50
      ) -> List[Dict]:
      ```
    - Get tiktoken encoding: encoding = tiktoken.get_encoding("cl100k_base")
    - Flatten pages into continuous text stream while preserving position info:
      - Build list of (text, page_num, offset) tuples from pages
      - Concatenate all text for tokenization
    - Encode full text: tokens = encoding.encode(full_text)
    - Chunk tokens in sliding window:
      - Start at index 0
      - Each chunk: tokens[start:start + max_tokens]
      - Decode chunk back to text: encoding.decode(chunk_tokens)
      - Find which page this chunk belongs to (based on char offsets)
      - Build chunk dict:
        ```python
        {
            "text": chunk_text,
            "chunk_index": i,
            "token_count": len(chunk_tokens),
            "page_num": page_num,
            "char_offset": offset,
            "doc_id": doc_id  # Added by caller
        }
        ```
      - Move start forward by (max_tokens - overlap) for next iteration
    - Return list of chunk dicts
    - Handle edge case: If text is empty, return empty list
    - Add docstring explaining token-aware splitting for better ML results

    Do NOT split by sentences/paragraphs yet (semantic splitting deferred to Phase 4 with embeddings).
  </action>
  <verify>
    Test with sample text: chunks should be ~500 tokens each
    Verify overlap: Adjacent chunks should share ~50 tokens
    Check page_num: Each chunk correctly maps to source page
    Check token_count: No chunk exceeds 500 tokens
  </verify>
  <done>
    chunk_text_by_tokens() returns list of chunks with proper metadata, each under 500 tokens.
  </done>
</task>

<task type="auto">
  <name>Task 3: Create PDF upload API endpoint</name>
  <files>
    python-service/api/pdf.py
    python-service/models/schemas.py
    python-service/models/__init__.py
    python-service/requirements.txt
  </files>
  <action>
    Create PDF upload endpoint:

    **python-service/models/__init__.py:**
    - Empty init file for models package

    **python-service/models/schemas.py:**
    - Import pydantic
    - Create UploadResponse model:
      ```python
      class UploadResponse(BaseModel):
          status: str
          doc_id: str
          filename: str
          chunk_count: int
          metadata: Dict[str, str]
      ```
    - Create Chunk model for chunk data structure:
      ```python
      class Chunk(BaseModel):
          text: str
          chunk_index: int
          token_count: int
          page_num: int
          char_offset: int
          doc_id: str
      ```

    **python-service/api/pdf.py:**
    - Import FastAPI, UploadFile, File, BackgroundTasks, HTTPException
    - Import pdf_extractor, chunker, schemas, uuid, pathlib, shutil
    - Create FastAPI router: router = APIRouter(prefix="/api/pdf")
    - Define UPLOAD_DIR as Path("/tmp/doc-research-uploads") (create if not exists)
    - Implement POST /upload endpoint:
      - Validate file extension (.pdf only, raise 400 if not)
      - Generate doc_id using uuid4()
      - Save uploaded file to UPLOAD_DIR / {doc_id}.pdf
      - Extract pages: pages_data = extract_pdf_pages(file_path)
      - Chunk text: chunks = chunk_text_by_tokens(pages_data["pages"])
      - Add doc_id to each chunk dict
      - Return UploadResponse with:
        - status: "processed" (note: not stored yet, that's Plan 02-03)
        - doc_id: generated UUID
        - filename: original filename
        - chunk_count: len(chunks)
        - metadata: pages_data["metadata"]
    - Note: Storage will be added in Plan 02-03 (ChromaDB integration)

    **Update python-service/main.py:**
    - Import PDF router: `from api.pdf import router as pdf_router`
    - Include router: `app.include_router(pdf_router, tags=["pdf"])`

    **Update python-service/requirements.txt:**
    - Add: PyMuPDF==1.24.12
    - Add: tiktoken==0.8.0

    Do NOT process large PDFs synchronously (will move to background tasks in Plan 02-03).
  </action>
  <verify>
    Start service: `cd python-service && python3 main.py`
    Upload test PDF: `curl -X POST -F "file=@test.pdf" http://127.0.0.1:PORT/api/pdf/upload`
    Verify response: Returns JSON with doc_id, chunk_count, metadata
    Check chunks: chunk_count > 0 for multi-page PDF
  </verify>
  <done>
    PDF upload endpoint accepts files, extracts text, chunks by tokens, returns structured response.
  </done>
</task>

</tasks>

<verification>
1. PDF upload endpoint accepts .pdf files and rejects other formats
2. Text extraction works on multi-page PDFs with correct page numbers
3. Chunking produces ~500 token chunks with 50 token overlap
4. Each chunk includes metadata (doc_id, page_num, chunk_index, offset)
5. Upload response returns valid JSON with all expected fields
</verification>

<success_criteria>
PDF processing pipeline ready for:
- ChromaDB vector storage (Plan 02-03)
- Background task processing (Plan 02-03)
</success_criteria>

<output>
After completion, create `.planning/phases/02-python-ml-service/02-02-SUMMARY.md`
</output>
