---
phase: 02-python-ml-service
plan: 03
type: execute
wave: 3
depends_on: [02-02]
files_modified:
  - python-service/services/vector_store.py
  - python-service/requirements.txt
  - python-service/api/pdf.py
  - python-service/config.py
autonomous: true

must_haves:
  truths:
    - "ChromaDB client persists data to disk in app data directory"
    - "Document chunks are stored in 'documents' collection with metadata"
    - "Chunks can be queried by doc_id filter"
    - "Data survives service restart (persistent storage)"
  artifacts:
    - path: "python-service/services/vector_store.py"
      provides: "ChromaDB persistent client and collection operations"
      exports: ["get_chroma_client", "get_documents_collection", "add_document_chunks", "query_by_document"]
    - path: "python-service/config.py"
      provides: "Platform-specific app data path configuration"
      exports: ["get_app_data_path"]
    - path: "python-service/requirements.txt"
      provides: "Python dependencies"
      contains: ["chromadb"]
  key_links:
    - from: "python-service/services/vector_store.py"
      to: "chromadb.PersistentClient"
      via: "import statement"
      pattern: "import chromadb|from chromadb"
    - from: "python-service/api/pdf.py"
      to: "python-service/services/vector_store.py"
      via: "import and function call"
      pattern: "from.*vector_store.*import|add_document_chunks"
    - from: "python-service/config.py"
      to: "platform module"
      via: "import statement"
      pattern: "import platform"
---

<objective>
ChromaDB vector storage integration with persistent client for document chunks.

Purpose: Store extracted PDF chunks in vector database for semantic search in Phase 4.
Output: Persistent ChromaDB collection with document chunks and metadata.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/phases/02-python-ml-service/02-CONTEXT.md
@.planning/phases/02-python-ml-service/02-RESEARCH.md
@.planning/phases/02-python-ml-service/02-02-SUMMARY.md

# Application settings location from project instructions
$HOME/Library/Application\ Support/document-research/
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create ChromaDB storage configuration</name>
  <files>
    python-service/config.py
  </files>
  <action>
    Add ChromaDB path configuration to config.py:

    - Import platform and pathlib modules
    - Implement get_app_data_path() function:
      ```python
      def get_app_data_path() -> Path:
          """Get platform-appropriate application data directory."""
          app_name = "document-research"
          system = platform.system()

          if system == "Darwin":  # macOS
              base = Path.home() / "Library" / "Application Support" / app_name
          elif system == "Windows":
              base = Path.home() / "AppData" / "Local" / app_name
          else:  # Linux and others
              base = Path.home() / ".local" / "share" / app_name

          # Create directory if it doesn't exist
          base.mkdir(parents=True, exist_ok=True)
          return base
      ```
    - Add CHROMA_DIR constant: get_app_data_path() / "chroma"
    - Add UPLOAD_DIR constant: Path("/tmp/doc-research-uploads")
    - Update existing config constants to use these paths

    Do NOT use working directory for storage (data would be lost after packaging).
  </action>
  <verify>
    Test on macOS: Should return ~/Library/Application Support/document-research
    Test directory creation: Directory should be created if it doesn't exist
    Verify write permissions: Should be able to create files in the directory
  </verify>
  <done>
    get_app_data_path() returns correct platform-specific path with directory created.
  </done>
</task>

<task type="auto">
  <name>Task 2: Create ChromaDB vector store service</name>
  <files>
    python-service/services/vector_store.py
    python-service/requirements.txt
  </files>
  <action>
    Create ChromaDB integration service:

    **python-service/services/vector_store.py:**
    - Import chromadb, typing (List, Dict, Optional), pathlib
    - Import config: from config import CHROMA_DIR
    - Implement get_chroma_client() function:
      - Use chromadb.PersistentClient with path from CHROMA_DIR
      - Return client instance (lazy initialization, cached in module)
      - Create CHROMA_DIR if it doesn't exist
    - Implement get_documents_collection() function:
      - Get client using get_chroma_client()
      - Get or create collection named "documents"
      - Set metadata: {"hnsw:space": "cosine"} for cosine similarity
      - Return collection instance
    - Implement add_document_chunks(collection, doc_id: str, chunks: List[Dict]):
      - Extract arrays from chunks list:
        - documents: [c["text"] for c in chunks]
        - ids: [f"{doc_id}_chunk_{c['chunk_index']}" for c in chunks]
        - metadatas: [{
            "doc_id": doc_id,
            "chunk_index": c["chunk_index"],
            "page_num": c.get("page_num", 0),
            "char_offset": c.get("char_offset", 0)
          } for c in chunks]
      - Call collection.add(documents=..., ids=..., metadatas=...)
      - Note: embeddings parameter omitted (will be added in Phase 4)
      - Return success status
    - Implement query_by_document(collection, doc_id: str, n_results: int = 10):
      - Call collection.query(
          query_texts=[""],  # Empty query for metadata-only retrieval
          n_results=n_results,
          where={"doc_id": doc_id}
        )
      - Return results dict with documents, metadatas, ids
      - Handle empty results gracefully
    - Implement delete_document(collection, doc_id: str):
      - Call collection.delete(where={"doc_id": doc_id})
      - Return count of deleted chunks
      - Note: Archival to separate collection deferred to future phase
    - Add docstrings for each function

    **Update python-service/requirements.txt:**
    - Add: chromadb==0.6.3

    Do NOT use ephemeral Client() (data would be lost on restart).
    Do NOT create separate collections per document (use single collection with doc_id filter).
  </action>
  <verify>
    Import test: Import chromadb and create PersistentClient successfully
    Collection test: get_documents_collection() returns collection with name "documents"
    Persistence check: ChromaDB files created in CHROMA_DIR after adding chunks
    Query test: Can query chunks by doc_id with where filter
  </verify>
  <done>
    vector_store.py provides ChromaDB client, collection access, add/query/delete operations.
  </done>
</task>

<task type="auto">
  <name>Task 3: Integrate ChromaDB with PDF upload endpoint</name>
  <files>
    python-service/api/pdf.py
  </files>
  <action>
    Update PDF upload endpoint to store chunks in ChromaDB:

    **python-service/api/pdf.py:**
    - Import vector_store: from services.vector_store import get_documents_collection, add_document_chunks
    - Import BackgroundTasks (already imported)
    - Create async function process_and_store_pdf(doc_id: str, file_path: str):
      - Extract pages: pages_data = extract_pdf_pages(file_path)
      - Chunk text: chunks = chunk_text_by_tokens(pages_data["pages"])
      - Add doc_id to each chunk
      - Get collection: collection = get_documents_collection()
      - Store chunks: add_document_chunks(collection, doc_id, chunks)
      - Log completion (number of chunks stored)
    - Update POST /upload endpoint:
      - Save uploaded file to disk (as before)
      - Add background task: background_tasks.add_task(process_and_store_pdf, doc_id, file_path)
      - Return UploadResponse with status "processing" (changed from "processed")
      - Include note that background task will handle storage
    - Add GET /documents/{doc_id} endpoint:
      - Get collection: get_documents_collection()
      - Query chunks: query_by_document(collection, doc_id, n_results=100)
      - Return JSON with chunks list and metadata
    - Add DELETE /documents/{doc_id} endpoint:
      - Get collection: get_documents_collection()
      - Delete chunks: delete_document(collection, doc_id)
      - Return JSON with deleted count
      - Note: Archival not implemented, simple delete for now

    Move PDF processing to background task to avoid blocking HTTP thread (prevents timeout on large files).
  </action>
  <verify>
    Upload PDF: curl POST /api/pdf/upload, returns "processing" status
    Wait 1-2 seconds for background task
    Query document: curl GET /api/pdf/documents/{doc_id}, returns chunks
    Check persistence: Restart service, query again, chunks still present
    Delete document: curl DELETE /api/pdf/documents/{doc_id}, chunks removed
  </verify>
  <done>
    PDF upload stores chunks in ChromaDB via background task, chunks persist across restarts.
  </done>
</task>

</tasks>

<verification>
1. ChromaDB storage directory created in app data folder
2. Document chunks stored in 'documents' collection with proper metadata
3. Chunks can be retrieved by doc_id filter
4. Data persists after service restart (stop Python, restart, query returns same data)
5. Background processing prevents HTTP timeout on large PDFs
6. Delete endpoint removes chunks from collection
</verification>

<success_criteria>
Vector storage complete and ready for:
- Electron integration (Plan 02-04)
- Phase 4: Embeddings and semantic search
</success_criteria>

<output>
After completion, create `.planning/phases/02-python-ml-service/02-03-SUMMARY.md`
</output>
